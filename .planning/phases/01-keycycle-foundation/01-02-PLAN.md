---
phase: 01-keycycle-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - hfs/tests/test_agno_providers.py
  - hfs/agno/providers.py
autonomous: false
user_setup:
  - service: multi-provider-llm
    why: "Testing requires real API keys for Cerebras, Groq, Gemini, and OpenRouter"
    env_vars:
      - name: NUM_CEREBRAS
        source: "Set to count of CEREBRAS_API_KEY_N variables in .env"
      - name: NUM_GROQ
        source: "Set to count of GROQ_API_KEY_N variables in .env"
      - name: NUM_GEMINI
        source: "Set to count of GEMINI_API_KEY_N variables in .env"
      - name: NUM_OPENROUTER
        source: "Set to count of OPENROUTER_API_KEY_N variables in .env"
      - name: TIDB_DB_URL
        source: "TiDB Cloud dashboard -> Connect -> Connection string"
    dashboard_config: []

must_haves:
  truths:
    - "Integration tests verify ProviderManager initializes with real keys"
    - "get_model() returns a model that can execute a simple prompt"
    - "Usage is logged to TiDB after API calls"
    - "Key rotation occurs automatically when rate limits hit (hard to test directly)"
  artifacts:
    - path: "hfs/tests/test_agno_providers.py"
      provides: "Integration tests for provider manager"
      contains: "test_provider_manager"
      min_lines: 60
  key_links:
    - from: "hfs/tests/test_agno_providers.py"
      to: "hfs/agno"
      via: "import"
      pattern: "from hfs\\.agno import"
    - from: "hfs/tests/test_agno_providers.py"
      to: "agno.agent"
      via: "Agent usage"
      pattern: "Agent\\("
---

<objective>
Create integration tests and verify end-to-end functionality of the Keycycle provider integration.

Purpose: Validates that the provider manager works with real API keys and TiDB persistence before moving to Phase 2.
Output: Working integration tests and verified provider initialization.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-keycycle-foundation/01-RESEARCH.md
@.planning/phases/01-keycycle-foundation/01-01-SUMMARY.md

# Prior plan created hfs/agno/ module
# Reference for API patterns and error types
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration test suite</name>
  <files>hfs/tests/test_agno_providers.py</files>
  <action>
Create comprehensive integration tests for the provider manager:

1. Create `hfs/tests/test_agno_providers.py`:

```python
"""Integration tests for HFS Agno provider integration.

These tests require real API keys configured in environment:
- NUM_CEREBRAS, CEREBRAS_API_KEY_1..N
- NUM_GROQ, GROQ_API_KEY_1..N
- NUM_GEMINI, GEMINI_API_KEY_1..N
- NUM_OPENROUTER, OPENROUTER_API_KEY_1..N
- TIDB_DB_URL (optional, for usage persistence)

Run with: pytest hfs/tests/test_agno_providers.py -v
"""

import os
import pytest
from unittest.mock import patch

from hfs.agno import (
    ProviderManager,
    get_model,
    get_provider_manager,
    list_available_providers,
    get_cerebras_model,
    get_groq_model,
    get_gemini_model,
    get_openrouter_model,
)


class TestProviderManagerInit:
    """Test ProviderManager initialization."""

    def test_provider_manager_creates_instance(self):
        """ProviderManager can be instantiated."""
        pm = ProviderManager()
        assert pm is not None
        assert hasattr(pm, 'wrappers')
        assert hasattr(pm, 'environment_status')

    def test_provider_configs_defined(self):
        """All four providers are configured."""
        from hfs.agno.providers import PROVIDER_CONFIGS
        providers = [p[0] for p in PROVIDER_CONFIGS]
        assert "cerebras" in providers
        assert "groq" in providers
        assert "gemini" in providers
        assert "openrouter" in providers

    def test_environment_status_populated(self):
        """Environment status contains all provider entries."""
        pm = ProviderManager()
        status = pm.environment_status
        assert "cerebras" in status
        assert "groq" in status
        assert "gemini" in status
        assert "openrouter" in status

    def test_available_providers_returns_list(self):
        """available_providers returns list of initialized providers."""
        pm = ProviderManager()
        providers = pm.available_providers
        assert isinstance(providers, list)


class TestEnvironmentValidation:
    """Test environment variable validation."""

    def test_missing_env_vars_logged(self, caplog):
        """Missing environment variables are logged as warnings."""
        with patch.dict(os.environ, {}, clear=True):
            import logging
            caplog.set_level(logging.WARNING)
            pm = ProviderManager()
            # Should have logged warnings about missing vars
            assert any("Missing" in record.message or "not set" in record.message
                      for record in caplog.records)

    def test_is_provider_healthy_false_when_not_configured(self):
        """is_provider_healthy returns False for unconfigured provider."""
        with patch.dict(os.environ, {}, clear=True):
            pm = ProviderManager()
            # Without env vars, providers won't be healthy
            # (may still partially initialize depending on defaults)
            assert hasattr(pm, 'is_provider_healthy')


class TestModelFactory:
    """Test model factory functions."""

    def test_get_provider_manager_singleton(self):
        """get_provider_manager returns singleton instance."""
        pm1 = get_provider_manager()
        pm2 = get_provider_manager()
        assert pm1 is pm2

    def test_list_available_providers_callable(self):
        """list_available_providers returns list."""
        providers = list_available_providers()
        assert isinstance(providers, list)


# Integration tests that require real API keys
# Mark as integration so they can be skipped in CI

@pytest.mark.integration
class TestRealProviders:
    """Integration tests requiring real API keys.

    Skip with: pytest -m "not integration"
    """

    @pytest.fixture(autouse=True)
    def check_env(self):
        """Skip if required env vars not set."""
        required = ["NUM_CEREBRAS", "NUM_GROQ", "NUM_GEMINI", "NUM_OPENROUTER"]
        missing = [v for v in required if not os.environ.get(v)]
        if missing:
            pytest.skip(f"Missing environment variables: {missing}")

    def test_cerebras_model_responds(self):
        """Cerebras model can respond to simple prompt."""
        from agno.agent import Agent
        model = get_cerebras_model()
        agent = Agent(model=model, instructions="Respond with just 'OK'")
        response = agent.run("Say OK")
        assert response is not None

    def test_groq_model_responds(self):
        """Groq model can respond to simple prompt."""
        from agno.agent import Agent
        model = get_groq_model()
        agent = Agent(model=model, instructions="Respond with just 'OK'")
        response = agent.run("Say OK")
        assert response is not None

    def test_gemini_model_responds(self):
        """Gemini model can respond to simple prompt."""
        from agno.agent import Agent
        model = get_gemini_model()
        agent = Agent(model=model, instructions="Respond with just 'OK'")
        response = agent.run("Say OK")
        assert response is not None

    def test_openrouter_model_responds(self):
        """OpenRouter model can respond to simple prompt."""
        from agno.agent import Agent
        model = get_openrouter_model()
        agent = Agent(model=model, instructions="Respond with just 'OK'")
        response = agent.run("Say OK")
        assert response is not None

    def test_get_model_with_custom_model_id(self):
        """get_model accepts custom model_id parameter."""
        # Use Cerebras with a different model
        model = get_model("cerebras", model_id="llama3.1-8b")
        assert model is not None

    def test_provider_manager_shutdown(self):
        """ProviderManager.shutdown() completes without error."""
        pm = ProviderManager()
        pm.shutdown()
        # Should not raise


@pytest.mark.integration
class TestTiDBPersistence:
    """Test usage persistence to TiDB."""

    @pytest.fixture(autouse=True)
    def check_tidb(self):
        """Skip if TIDB_DB_URL not set."""
        if not os.environ.get("TIDB_DB_URL"):
            pytest.skip("TIDB_DB_URL not set")

    def test_usage_persisted_after_call(self):
        """Usage data should be persisted after API call.

        Note: This is hard to verify directly without querying TiDB.
        The test just confirms the call completes without error.
        """
        from agno.agent import Agent
        model = get_cerebras_model()
        agent = Agent(model=model, instructions="Respond briefly")
        response = agent.run("Hello")
        # If we get here without error, persistence is at least not breaking
        assert response is not None
```

2. Add pytest marker configuration to `hfs/pyproject.toml` (if not already present):
   - Add `[tool.pytest.ini_options]` section with `markers = ["integration: marks tests as integration tests"]`
  </action>
  <verify>
```bash
cd C:\Users\jinwi\programming_files_NEW\agentive
# Run unit tests only (skip integration)
python -m pytest hfs/tests/test_agno_providers.py -v -m "not integration" --tb=short 2>&1 | head -40
```
Unit tests should pass (the non-integration ones that don't require real keys).
  </verify>
  <done>
Integration test file exists with unit tests that pass without API keys and integration tests marked for real provider testing.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add graceful degradation for missing providers</name>
  <files>hfs/agno/providers.py</files>
  <action>
Enhance ProviderManager to handle partial provider availability gracefully:

1. Add a method to get a working model from ANY available provider (fallback pattern):
   ```python
   def get_any_model(self, estimated_tokens: int = 1000, **kwargs) -> tuple[str, Any]:
       """Get a model from any available provider.

       Returns (provider_name, model) tuple.
       Raises NoAvailableKeyError if no providers have keys available.

       Tries providers in order: cerebras, groq, gemini, openrouter
       """
       from keycycle import NoAvailableKeyError

       errors = []
       for provider in self.available_providers:
           try:
               model = self.get_model(provider, estimated_tokens=estimated_tokens, **kwargs)
               return (provider, model)
           except NoAvailableKeyError as e:
               errors.append((provider, e))
               continue
           except Exception as e:
               errors.append((provider, e))
               continue

       # All providers failed
       error_summary = "; ".join(f"{p}: {e}" for p, e in errors)
       raise NoAvailableKeyError(
           provider="all",
           model_id="any",
           total_keys=0,
           cooling_down=0,
       ) from Exception(f"All providers exhausted: {error_summary}")
   ```

2. Add a summary method for debugging:
   ```python
   def get_status_summary(self) -> str:
       """Get a human-readable status summary."""
       lines = ["Provider Status:"]
       for provider, status in self.environment_status.items():
           configured = "Yes" if status["configured"] else "No"
           initialized = "Yes" if provider in self.wrappers else "No"
           lines.append(f"  {provider}: configured={configured}, initialized={initialized}, keys={status['keys']}")

       tidb = "Yes" if os.environ.get("TIDB_DB_URL") else "No"
       lines.append(f"  TiDB persistence: {tidb}")

       return "\n".join(lines)
   ```

3. Log the status summary at INFO level during __init__ (after _init_providers):
   ```python
   logger.info(self.get_status_summary())
   ```
  </action>
  <verify>
```bash
cd C:\Users\jinwi\programming_files_NEW\agentive
python -c "
from hfs.agno.providers import ProviderManager
pm = ProviderManager()
print(pm.get_status_summary())
print()
print('get_any_model method exists:', hasattr(pm, 'get_any_model'))
"
```
Should show status summary and confirm get_any_model exists.
  </verify>
  <done>
ProviderManager has get_any_model for fallback model acquisition and get_status_summary for debugging. Status logged at initialization.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Keycycle provider integration with:
- ProviderManager class managing 4 LLM providers
- Model factory with convenience functions
- Environment validation and status reporting
- Integration test suite
- Graceful fallback for partial availability
  </what-built>
  <how-to-verify>
1. Ensure your .env file has the required variables:
   - NUM_CEREBRAS, CEREBRAS_API_KEY_1..N
   - NUM_GROQ, GROQ_API_KEY_1..N
   - NUM_GEMINI, GEMINI_API_KEY_1..N
   - NUM_OPENROUTER, OPENROUTER_API_KEY_1..N
   - TIDB_DB_URL

2. Run the full integration test:
   ```bash
   cd C:\Users\jinwi\programming_files_NEW\agentive
   python -m pytest hfs/tests/test_agno_providers.py -v --tb=short
   ```

3. Quick manual verification:
   ```bash
   python -c "
   from hfs.agno import get_cerebras_model
   from agno.agent import Agent

   model = get_cerebras_model()
   agent = Agent(model=model, instructions='Be brief')
   response = agent.run('Say hello')
   print('Response:', response.content if hasattr(response, 'content') else response)
   "
   ```

Expected: All tests pass, manual verification shows a response from Cerebras.
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass and manual verification works, or describe any issues encountered.</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:

1. Full test suite:
```bash
python -m pytest hfs/tests/test_agno_providers.py -v
```

2. Import verification:
```bash
python -c "from hfs.agno import ProviderManager, get_model, get_any_model; print('All exports OK')"
```

3. Status check:
```bash
python -c "from hfs.agno import get_provider_manager; print(get_provider_manager().get_status_summary())"
```
</verification>

<success_criteria>
- [ ] Integration tests exist in `hfs/tests/test_agno_providers.py`
- [ ] Unit tests pass without API keys
- [ ] Integration tests pass with API keys (user verified)
- [ ] get_any_model provides fallback across providers
- [ ] Status summary shows all 4 providers
- [ ] Manual verification confirms model responds
</success_criteria>

<output>
After completion, create `.planning/phases/01-keycycle-foundation/01-02-SUMMARY.md`
</output>
