---
phase: 10-textual-core
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - hfs/tui/app.py
  - hfs/tui/screens/__init__.py
  - hfs/tui/screens/chat.py
  - hfs/tui/widgets/__init__.py
  - hfs/tui/widgets/chat_input.py
  - hfs/tui/widgets/message_list.py
  - hfs/tui/widgets/message.py
  - hfs/tui/widgets/spinner.py
autonomous: true

must_haves:
  truths:
    - "User can type a message and submit with Enter"
    - "Assistant responses stream token-by-token with markdown rendering"
    - "Messages scroll up as new ones arrive; input stays at bottom"
    - "Slash commands /help, /clear, /exit work"
    - "Pulsing dot indicator shows during streaming"
    - "Shift+Enter inserts newline in input"
  artifacts:
    - path: "hfs/tui/screens/chat.py"
      provides: "ChatScreen with message list and input"
      min_lines: 80
    - path: "hfs/tui/widgets/chat_input.py"
      provides: "ChatInput widget with Enter/Shift+Enter handling"
      min_lines: 40
    - path: "hfs/tui/widgets/message_list.py"
      provides: "MessageList container with smart scroll"
      min_lines: 30
    - path: "hfs/tui/widgets/message.py"
      provides: "ChatMessage widget with markdown rendering"
      min_lines: 50
    - path: "hfs/tui/widgets/spinner.py"
      provides: "PulsingDot streaming indicator"
      min_lines: 20
  key_links:
    - from: "hfs/tui/screens/chat.py"
      to: "hfs/tui/widgets/chat_input.py"
      via: "import and compose"
      pattern: "ChatInput"
    - from: "hfs/tui/screens/chat.py"
      to: "hfs/tui/widgets/message_list.py"
      via: "import and compose"
      pattern: "MessageList"
    - from: "hfs/tui/widgets/message.py"
      to: "textual.widgets.Markdown"
      via: "streaming rendering"
      pattern: "Markdown"
---

<objective>
Build the chat interface widgets: input field, message list, message rendering with streaming markdown, and slash command handling.

Purpose: This is the core chat experience. Users type messages, see them appear in a scrolling list, and watch assistant responses stream in with live markdown rendering.

Output: Functional chat screen with streaming responses, slash commands, and proper scroll behavior
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-textual-core/10-CONTEXT.md
@.planning/phases/10-textual-core/10-RESEARCH.md

# Prior plan output
@.planning/phases/10-textual-core/10-01-SUMMARY.md

# Existing app to extend
@hfs/tui/app.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat input and message widgets</name>
  <files>
    hfs/tui/widgets/__init__.py
    hfs/tui/widgets/chat_input.py
    hfs/tui/widgets/message.py
    hfs/tui/widgets/message_list.py
    hfs/tui/widgets/spinner.py
  </files>
  <action>
1. Create hfs/tui/widgets/__init__.py:
   - Export ChatInput, ChatMessage, MessageList, PulsingDot
   - Add __all__ list

2. Create hfs/tui/widgets/chat_input.py (ChatInput class):
   - Extend TextArea (not Input - we need multi-line with auto-grow)
   - BINDINGS: Binding("enter", "submit", show=False), Binding("shift+enter", "newline", show=False)
   - action_submit(): Get text, strip it, if not empty post Submitted message and clear
   - action_newline(): Insert "\n" at cursor position
   - Define Submitted(Message) class with text attribute
   - DEFAULT_CSS: height auto, max-height 10 lines, border styling
   - Character counter in watch method (reactive count displayed somewhere - or defer to status bar)

3. Create hfs/tui/widgets/message.py (ChatMessage class):
   - Extend Static (container for message content)
   - Constructor takes: content (str), is_user (bool), timestamp (datetime optional)
   - Compose yields: role label (Static), Markdown widget for content
   - is_user determines styling: user messages right-aligned or different color
   - For assistant messages, expose markdown widget for streaming updates
   - Method: stream_content() returns the Markdown widget for streaming
   - Method: append_content(text) for streaming - uses Markdown.get_stream() pattern from research

4. Create hfs/tui/widgets/message_list.py (MessageList class):
   - Extend VerticalScroll for scrollable container
   - on_mount(): Call anchor() for auto-scroll behavior
   - async add_message(content, is_user) -> ChatMessage: Mount message, return it
   - async add_streaming_message() -> ChatMessage: Mount empty assistant message, return for streaming
   - Smart scroll: Only auto-scroll if at bottom (check is_vertical_scrollbar_grabbed)

5. Create hfs/tui/widgets/spinner.py (PulsingDot class):
   - Extend Static
   - Reactive is_pulsing: bool = False
   - on_mount(): set_interval(0.5, self._pulse)
   - _pulse(): If is_pulsing, animate opacity between 0.3 and 1.0
   - render(): Return "..." if is_pulsing else ""
   - DEFAULT_CSS: color $primary (will use theme color)

Follow RESEARCH.md patterns exactly - especially the Enter vs Shift+Enter handling and Markdown streaming.
  </action>
  <verify>
    - `python -c "from hfs.tui.widgets import ChatInput, ChatMessage, MessageList, PulsingDot; print('OK')"` prints OK
    - No import errors
  </verify>
  <done>
    - All widget classes exist and can be imported
    - ChatInput has Enter submit / Shift+Enter newline bindings
    - ChatMessage supports markdown content and streaming
    - MessageList has smart scroll behavior
    - PulsingDot animates when is_pulsing=True
  </done>
</task>

<task type="auto">
  <name>Task 2: Create chat screen with slash commands and wire to app</name>
  <files>
    hfs/tui/screens/__init__.py
    hfs/tui/screens/chat.py
    hfs/tui/app.py
  </files>
  <action>
1. Create hfs/tui/screens/__init__.py:
   - Export ChatScreen
   - Add __all__ list

2. Create hfs/tui/screens/chat.py (ChatScreen class):
   - Extend Screen
   - compose() yields: MessageList (id="messages"), PulsingDot (id="spinner"), ChatInput (id="input")
   - Layout: messages fills space, spinner inline, input at bottom (use CSS or Container)

   - SLASH_COMMANDS dict mapping command -> method name:
     "/help": "show_help"
     "/clear": "clear_conversation"
     "/exit": "quit_app"

   - on_chat_input_submitted(event: ChatInput.Submitted):
     - Get text from event
     - If starts with "/", handle slash command
     - Else, call send_message(text)

   - async send_message(text: str):
     - Add user message to message list
     - Show pulsing dot
     - Create streaming assistant message
     - TODO: Actually call LLM (for now, simulate with mock response)
     - Stream mock response character by character with small delays
     - Hide pulsing dot when done

   - show_help(): Add system message listing available commands
   - clear_conversation(): Clear all messages from MessageList (remove all children)
   - quit_app(): Call self.app.exit()

   - For mock streaming (placeholder until LLM integration):
     Use @work decorator for async streaming
     Simulate typing with asyncio.sleep(0.02) between characters
     Stream a sample markdown response with code block to test rendering

3. Update hfs/tui/app.py:
   - Import ChatScreen
   - In compose() or on_mount(), push ChatScreen as the main screen
   - Or use SCREENS class variable if appropriate
   - The app should now show the chat interface on launch

The streaming is mock for now - real LLM integration comes later. Focus on the UI working correctly.
  </action>
  <verify>
    - `hfs` launches and shows chat interface
    - Typing text and pressing Enter shows user message
    - Mock assistant response streams in
    - `/help` shows help message
    - `/clear` clears messages
    - `/exit` quits the app
    - Shift+Enter inserts newline in input
  </verify>
  <done>
    - ChatScreen exists with message list, input, and spinner
    - Slash commands /help, /clear, /exit all work
    - User messages appear when submitted
    - Mock streaming response demonstrates markdown rendering
    - Pulsing dot shows during streaming
  </done>
</task>

</tasks>

<verification>
All verification for this plan:
1. `hfs` launches chat interface (not placeholder)
2. Type "Hello" and press Enter - user message appears
3. Mock assistant response streams in with markdown
4. `/help` shows available commands
5. `/clear` removes all messages
6. `/exit` quits the application
7. Shift+Enter in input creates newline
8. Messages scroll automatically but stop if user scrolls up
9. Pulsing dot visible during response streaming
</verification>

<success_criteria>
- Chat interface with input at bottom, messages scrolling up
- Enter submits, Shift+Enter creates newline
- Mock streaming responses demonstrate markdown rendering
- Slash commands /help, /clear, /exit functional
- Pulsing dot indicator during streaming
- Smart scroll behavior (auto-scroll unless user scrolled up)
</success_criteria>

<output>
After completion, create `.planning/phases/10-textual-core/10-02-SUMMARY.md`
</output>
