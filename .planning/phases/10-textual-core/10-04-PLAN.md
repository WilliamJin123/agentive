---
phase: 10-textual-core
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - hfs/tui/screens/chat.py
  - hfs/tui/app.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "User messages are sent to real LLM and responses stream back token-by-token"
    - "LLM errors display gracefully in chat with retry option"
    - "Status bar updates with actual token usage from LLM responses"
  artifacts:
    - path: "hfs/tui/screens/chat.py"
      provides: "Real LLM integration with streaming"
      contains: "ProviderManager"
    - path: "hfs/tui/app.py"
      provides: "ProviderManager initialization"
      contains: "provider_manager"
  key_links:
    - from: "hfs/tui/screens/chat.py"
      to: "hfs/agno/providers.py"
      via: "ProviderManager.get_any_model()"
      pattern: "provider_manager\\.get_any_model"
    - from: "hfs/tui/screens/chat.py"
      to: "Agno Agent"
      via: "agent.arun_stream()"
      pattern: "arun_stream|async for.*chunk"
---

<objective>
Wire ChatScreen to real LLM via ProviderManager for token-by-token streaming responses.

Purpose: Close the verification gap where ChatScreen only streams mock data. Success criteria requires "LLM responses stream token-by-token" with actual LLM calls, not hardcoded strings.

Output: ChatScreen that sends user messages to a real LLM and streams actual responses.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-textual-core/10-VERIFICATION.md
@hfs/tui/screens/chat.py
@hfs/tui/app.py
@hfs/agno/providers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize ProviderManager in HFSApp</name>
  <files>hfs/tui/app.py</files>
  <action>
Add ProviderManager initialization to HFSApp:

1. Import ProviderManager from hfs.agno.providers
2. In HFSApp.__init__, create self.provider_manager = None (lazy init)
3. Add a method `get_provider_manager()` that:
   - Returns self.provider_manager if already initialized
   - Otherwise creates ProviderManager() and stores it
   - Handles exceptions gracefully (log error, return None)
4. This lazy initialization avoids blocking app startup on env validation

Pattern to follow:
```python
from hfs.agno.providers import ProviderManager

class HFSApp(App):
    def __init__(self):
        super().__init__()
        self._provider_manager: ProviderManager | None = None

    def get_provider_manager(self) -> ProviderManager | None:
        if self._provider_manager is None:
            try:
                self._provider_manager = ProviderManager()
            except Exception as e:
                self.log.error(f"Failed to init ProviderManager: {e}")
                return None
        return self._provider_manager
```

Do NOT import at module level to avoid slow startup.
  </action>
  <verify>
- `python -c "from hfs.tui.app import HFSApp; app = HFSApp(); print('OK')"` succeeds
- HFSApp has get_provider_manager method
  </verify>
  <done>HFSApp can lazily initialize ProviderManager on demand</done>
</task>

<task type="auto">
  <name>Task 2: Replace mock streaming with real LLM calls</name>
  <files>hfs/tui/screens/chat.py</files>
  <action>
Modify ChatScreen to use real LLM via Agno:

1. Import Agno Agent: `from agno import Agent`
2. Remove or keep _stream_mock_response as fallback when provider unavailable

3. Create new method `_stream_llm_response()` that:
   a. Gets provider_manager from self.app.get_provider_manager()
   b. If None (providers not configured), fall back to mock with system message explaining
   c. Gets model via `provider_manager.get_any_model(estimated_tokens=2000)`
   d. Creates Agno Agent with the model: `agent = Agent(model=model, system="You are HFS, a helpful assistant.")`
   e. Streams response via `async for chunk in agent.arun_stream(text):`
      - Extract text from chunk: `chunk.content[0].text` if chunk.content else ""
      - Append to message widget
      - Count tokens for status bar
   f. Handle exceptions gracefully - show error in chat, offer retry

4. Update `send_message()` to call `_stream_llm_response()` instead of `_stream_mock_response()`

5. Add error handling for common cases:
   - NoAvailableKeyError: "All API keys exhausted. Please wait or add more keys."
   - Network errors: "Connection error. Check your internet and retry."
   - Other exceptions: Show error message with traceback option

6. Token counting: Agno responses include usage info, extract and update status bar

Pattern for streaming:
```python
@work
async def _stream_llm_response(self, message: ChatMessage, spinner: PulsingDot, status_bar: HFSStatusBar, user_text: str) -> None:
    provider_manager = self.app.get_provider_manager()
    if provider_manager is None:
        # Fallback to mock with explanation
        await message.append_content("*LLM providers not configured. Showing mock response.*\n\n")
        # ... mock content
        return

    try:
        provider, model = provider_manager.get_any_model(estimated_tokens=2000)
        agent = Agent(model=model, system="You are HFS, a helpful AI assistant.")

        async for chunk in agent.arun_stream(user_text):
            if chunk.content:
                text = chunk.content[0].text if hasattr(chunk.content[0], 'text') else str(chunk.content[0])
                await message.append_content(text)

    except NoAvailableKeyError:
        await message.append_content("\n\n*Error: All API keys exhausted. Please wait or configure more keys.*")
    except Exception as e:
        await message.append_content(f"\n\n*Error: {e}*")
    finally:
        spinner.is_pulsing = False
```

Note: Check Agno's streaming API - it may be `agent.run_stream()` not `arun_stream()`. Verify with actual agno package.
  </action>
  <verify>
- `hfs` launches without error
- Typing a message and pressing Enter attempts LLM call (may fail if no keys configured, but should show error gracefully)
- If providers configured: Real LLM response streams token-by-token
- If providers not configured: Mock response with explanation message
  </verify>
  <done>ChatScreen sends user messages to real LLM and streams responses; errors handled gracefully</done>
</task>

</tasks>

<verification>
1. Run `hfs` and verify app launches
2. Type a message and press Enter
3. Verify either:
   - Real LLM response streams if providers configured
   - Mock response with "LLM providers not configured" message if not
4. Verify error messages display gracefully in chat (not crashes)
5. Verify status bar token count updates
</verification>

<success_criteria>
- ChatScreen uses ProviderManager.get_any_model() for LLM access
- Responses stream token-by-token from real LLM
- Graceful fallback to mock when providers unavailable
- Errors displayed in chat with clear messages
- No crashes on LLM failures
</success_criteria>

<output>
After completion, create `.planning/phases/10-textual-core/10-04-SUMMARY.md`
</output>
